{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel Collections\n",
    "---------------------\n",
    "\n",
    "Systems like Spark and Dask include \"big data\" collections with a small set of high-level primitives like `map`, `filter`, `groupby`, and `join`.  With these common patterns we can often handle computations that are more complex than map, but are still structured.\n",
    "\n",
    "In this section we repeat the submit example using the PySpark and the Dask.Bag APIs, which both provide parallel operations on linear collections of arbitrary objects.\n",
    "\n",
    "\n",
    "### Objectives\n",
    "\n",
    "*  Use high-level `pyspark` and `dask.bag` to parallelize common non-map patterns\n",
    "\n",
    "### Requirements\n",
    "\n",
    "*  PySpark\n",
    "\n",
    "*Note: PySpark requires a little additional setup. Usually, the following environment variables need to be set (using `/usr/libexec/java_home` on OS X or similar on Linux)*:\n",
    "\n",
    "```\n",
    "JAVA_HOME\n",
    "JAVA_JRE\n",
    "```\n",
    "\n",
    "*Also, VPNs can interfere with PySpark, so you may need to disable yours if you are running PySpark locally.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "We again start with the following sequential code\n",
    "\n",
    "```python\n",
    "series = {}\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series[fn] = pd.read_hdf(fn)['x']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for a in filenames:    # Doubly nested loop over the same collection\n",
    "    for b in filenames:  \n",
    "        if a != b:     # Filter out bad elements\n",
    "            results[a, b] = series[a].corr(series[b])  # Apply function\n",
    "\n",
    "((a, b), corr) = max(results.items(), key=lambda kv: kv[1])  # Reduction\n",
    "```\n",
    "\n",
    "### Spark/Dask.bag methods\n",
    "\n",
    "We can construct most of the above computation with the following Spark/Dask.bag methods:\n",
    "\n",
    "*  `collection.map(function)`: apply function to each element in collection\n",
    "*  `collection.cartesian(collection)`: Create new collection with every pair of inputs\n",
    "*  `collection.filter(predicate)`: Keep only elements of colleciton that match the predicate function\n",
    "*  `collection.max()`: Compute maximum element\n",
    "\n",
    "We use these briefly in isolated exercises and then combine them to rewrite the previous computation from the `submit` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark: Example API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local[4]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(5))  # create collection\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()  # Gather results back to local process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Square each element\n",
    "\n",
    "rdd.map(lambda x: x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Square each element and collect results\n",
    "\n",
    "rdd.map(lambda x: x ** 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the even elements\n",
    "\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (3, 0),\n",
       " (4, 0),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (3, 2),\n",
       " (4, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (4, 3),\n",
       " (4, 4)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cartesian product of each pair of elements in two sequences (or the same sequence in this case)\n",
    "\n",
    "rdd.cartesian(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (16, 2),\n",
       " (16, 3),\n",
       " (16, 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain operations to construct more complex computations\n",
    "\n",
    "(rdd.map(lambda x: x ** 2)\n",
    "    .cartesian(rdd)\n",
    "    .filter(lambda tup: tup[0] % 2 == 0)\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Parallelize pairwise correlations with PySpark\n",
    "\n",
    "To make this a bit easier we're just going to compute the maximum correlation and not try to keep track of the stocks that yielded this maximal result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/json/afl.h5',\n",
       " '../data/json/aig.h5',\n",
       " '../data/json/al.h5',\n",
       " '../data/json/avy.h5',\n",
       " '../data/json/bwa.h5']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "filenames = sorted(glob(os.path.join('..', 'data', 'json', '*.h5')))  # ../data/json/*.json\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 144 ms, total: 1.79 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Sequential Code\n",
    "\n",
    "series = []\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series.append(pd.read_hdf(fn)['close'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for a in series:    # Doubly nested loop over the same collection\n",
    "    for b in series:  \n",
    "        if not (a == b).all():     # Filter out comparisons of the same series \n",
    "            results.append(a.corr(b))  # Apply function\n",
    "\n",
    "result = max(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 4 ms, total: 20 ms\n",
      "Wall time: 7.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Parallel code\n",
    "\n",
    "rdd = sc.parallelize(filenames).map(lambda fn: pd.read_hdf(fn)[\"close\"])\n",
    "corr = (rdd\n",
    "     .cartesian(rdd)\n",
    "     .filter(lambda x: (x[0] != x[1]).any())\n",
    "     .map(lambda x: x[0].corr(x[1]))\n",
    "     .max()\n",
    ")\n",
    "\n",
    "result = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94646920143454671"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/collections-1.py\n",
    "### Parallel code\n",
    "\n",
    "rdd = sc.parallelize(filenames)\n",
    "series = rdd.map(lambda fn: pd.read_hdf(fn)['close'])\n",
    "\n",
    "corr = (series.cartesian(series)\n",
    "              .filter(lambda ab: not (ab[0] == ab[1]).all())\n",
    "              .map(lambda ab: ab[0].corr(ab[1]))\n",
    "              .max())\n",
    "\n",
    "result = corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask.bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/collections-2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import dask\n",
    "\n",
    "result = corr.compute(get=dask.local.get_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "*  Higher level collections include functions for common patterns\n",
    "*  Move data to collection, construct lazy computation, trigger at the end\n",
    "*  Used PySpark (`cartesian + map`) and Dask.bag (`product + map`) to handle nested for loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
